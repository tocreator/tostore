import '../handler/logger.dart';
import '../model/migration_meta.dart';
import '../model/migration_task.dart';
import '../model/table_schema.dart';
import '../model/meta_info.dart';
import 'compute_manager.dart';
import 'compute_tasks.dart';
import 'data_store_impl.dart';
import 'dart:convert';
import 'dart:async';
import 'dart:math';
import '../model/system_table.dart';
import '../Interface/chain_builder.dart';

/// Migration manager for handling database version upgrades
///
/// 1. Efficiently and automatically detect table structure changes
///   - Accurately detect field renaming, table renaming, and table deletion
///   - Use field ID or weight system to avoid misjudgment
///   - Table renaming detection based on multi-dimensional similarity analysis
///
/// 2. Optimized migration process
///   - Automatically generate migration tasks and execute them by space
///   - Transaction guarantee and error recovery mechanism
///   - Migration task persistence and status tracking
///
/// 3. High-performance data transformation
///   - Batch processing to optimize performance
///   - Priority processing for important spaces
///   - Memory management and cache optimization
class MigrationManager {
  final DataStoreImpl _dataStore;
  // Pending migration tasks
  final List<MigrationTask> _pendingTasks = [];
  // Whether migration tasks are being processed
  bool _isProcessingTasks = false;

  // In-memory cache for migration metadata to avoid frequent file reads
  MigrationMeta? _migrationMetaCache;
  // Current directory index cache (derived from directoryMapping)
  int? _currentDirIndexCache;
  // Lock for thread-safe access to cache
  Future<MigrationMeta>? _loadingFuture;

  MigrationManager(this._dataStore);

  /// Execute migration from old version to new version
  Future<void> migrate(List<TableSchema> schemas,
      {int batchSize = 1000}) async {
    try {
      Logger.info(
        'Start database migration: involving ${schemas.length} tables',
        label: 'MigrationManager.migrate',
      );

      // Performance optimization: Skip migration if no schemas
      if (schemas.isEmpty) {
        return;
      }

      // Get all existing tables
      final existingTables = await _dataStore.getTableNames();
      final allTasks = <MigrationTask>[];

      // Record migration start time
      final startTime = DateTime.now();

      // 1. First detect table renaming to avoid subsequent processing misidentifying renamed tables as new + deleted
      final detectRenameResult =
          await _detectRenamedTables(existingTables, schemas);
      final renamedTables = detectRenameResult.renamedTables;
      final tablesToCreate = detectRenameResult.tablesToCreate;
      final tablesToDrop = detectRenameResult.tablesToDrop;

      // Handle renamed tables
      for (final entry in renamedTables.entries) {
        final oldTableName = entry.key;
        final newSchema = entry.value;

        try {
          // Get old table schema
          final oldSchema =
              await _dataStore.schemaManager?.getTableSchema(oldTableName);
          if (oldSchema == null) {
            continue;
          }

          // Generate rename table operation
          final operations = <MigrationOperation>[
            MigrationOperation(
              type: MigrationType.renameTable,
              newTableName: newSchema.name,
            ),
          ];

          // Compare possible structure changes after renaming
          final schemaOperations =
              _compareSchemasAndGenerateOperations(oldSchema, newSchema);

          // Merge operations
          operations.addAll(schemaOperations);

          if (!SystemTable.isSystemTable(oldTableName)) {
            Logger.info(
              'Table [$oldTableName -> ${newSchema.name}] generated ${operations.length} migration operations',
              label: 'MigrationManager.migrate',
            );
          }

          // Create migration task but do not process immediately
          final task = await addMigrationTask(oldTableName, operations,
              startProcessing: false, isAutoGenerated: true);
          allTasks.add(task);
        } catch (e, stack) {
          Logger.error(
            'Failed to handle table renaming [$oldTableName -> ${newSchema.name}]: $e\n$stack',
            label: 'MigrationManager.migrate',
          );
        }
      }

      // 2. Handle remaining tables
      int tablesUpdated = 0;
      int tablesCreated = 0;

      for (var schema in schemas) {
        try {
          // Skip already processed renamed tables
          if (renamedTables.values.any((s) => s.name == schema.name)) {
            continue;
          }

          if (existingTables.contains(schema.name) &&
              !tablesToDrop.contains(schema.name)) {
            // Migrate existing table
            final task =
                await _migrateExistingTable(schema, batchSize: batchSize);
            if (task != null) {
              allTasks.add(task);
              tablesUpdated++;
            }
          } else if (tablesToCreate.contains(schema.name)) {
            // Create new table
            await _dataStore.createTable(schema);
            tablesCreated++;

            if (!SystemTable.isSystemTable(schema.name)) {
              Logger.info(
                'Create new table: ${schema.name}',
                label: 'MigrationManager.migrate',
              );
            }
          }
        } catch (e, stack) {
          Logger.error(
            'Failed to handle table [${schema.name}]: $e\n$stack',
            label: 'MigrationManager.migrate',
          );
        }
      }

      // 3. Handle tables to be deleted
      int tablesDropped = 0;
      for (final tableName in tablesToDrop) {
        try {
          // Skip old names of already processed renamed tables
          if (renamedTables.containsKey(tableName)) {
            continue;
          }

          // Create delete task for tables to be deleted
          final operations = <MigrationOperation>[
            const MigrationOperation(
              type: MigrationType.dropTable,
            ),
          ];

          final task = await addMigrationTask(tableName, operations,
              startProcessing: false, isAutoGenerated: true);
          allTasks.add(task);
          tablesDropped++;

          if (!SystemTable.isSystemTable(tableName)) {
            Logger.info(
              'Handle table deletion: $tableName',
              label: 'MigrationManager.migrate',
            );
          }
        } catch (e, stack) {
          Logger.error(
            'Failed to handle table deletion [$tableName]: $e\n$stack',
            label: 'MigrationManager.migrate',
          );
        }
      }

      // Start migration task processing
      if (allTasks.isNotEmpty) {
        Logger.info(
          'Database migration generated ${allTasks.length} migration tasks, starting execution...',
          label: 'MigrationManager.migrate',
        );
        final migrateSuccess = await processMigrationTasks();

        if (!migrateSuccess) {
          Logger.warn(
            'Some migration tasks failed, please check the log for details',
            label: 'MigrationManager.migrate',
          );
        } else {
          Logger.info(
            'All migration tasks have been successfully completed',
            label: 'MigrationManager.migrate',
          );
        }
      }

      // Calculate and record total duration
      final endTime = DateTime.now();
      final duration = endTime.difference(startTime);

      Logger.info(
        'Database migration completed:  Renamed tables [${renamedTables.length}], Updated tables [$tablesUpdated], New tables [$tablesCreated], Deleted tables [$tablesDropped], Total duration [${duration.inMilliseconds}ms]',
        label: 'MigrationManager.migrate',
      );
    } catch (e, stack) {
      Logger.error(
        'Database migration failed: $e\n$stack',
        label: 'MigrationManager.migrate',
      );
      rethrow;
    }
  }

  /// Detect renamed tables
  Future<RenamedTableResult> _detectRenamedTables(
      List<String> existingTables, List<TableSchema> newSchemas) async {
    try {
      // Result set
      final renamedTables = <String, TableSchema>{};
      final remainingExistingTables = List<String>.from(existingTables);
      final remainingNewTables = List<TableSchema>.from(newSchemas);

      // 0. First filter out tables with the same name, these do not need renaming matching
      final existingTableNames = Set<String>.from(remainingExistingTables);
      final newTableNames = remainingNewTables.map((s) => s.name).toSet();

      // Find tables with the same name and remove them from the matching list
      final commonTableNames = existingTableNames.intersection(newTableNames);
      if (commonTableNames.isNotEmpty) {
        // Remove tables with the same name from the matching list
        remainingExistingTables
            .removeWhere((name) => commonTableNames.contains(name));
        remainingNewTables
            .removeWhere((schema) => commonTableNames.contains(schema.name));
      }

      // Performance optimization: If after removing tables with the same name, all tables match (i.e., old and new tables are completely consistent), return the result directly
      if (remainingExistingTables.isEmpty && remainingNewTables.isEmpty) {
        return const RenamedTableResult(
          renamedTables: {},
          tablesToCreate: [],
          tablesToDrop: [],
        );
      }

      // 1. First match directly by tableId
      await _detectRenamedTablesByTableId(
          remainingExistingTables, remainingNewTables, renamedTables);

      // 2. Then match by similarity
      if (remainingExistingTables.isNotEmpty && remainingNewTables.isNotEmpty) {
        await _detectRenamedTablesBySimilarity(
            remainingExistingTables, remainingNewTables, renamedTables);
      }

      // 3. Determine tables to be created and to be deleted
      final tablesToCreate = <String>[];
      for (final schema in remainingNewTables) {
        tablesToCreate.add(schema.name);
      }

      final tablesToDrop = List<String>.from(remainingExistingTables);

      // Output detailed results
      if (renamedTables.isNotEmpty) {
        final userRenamedTables = renamedTables.entries
            .where((e) => !SystemTable.isSystemTable(e.key));
        if (userRenamedTables.isNotEmpty) {
          final renameInfo = userRenamedTables
              .map((e) => '${e.key} -> ${e.value.name}')
              .join(', ');
          Logger.info(
            'Final result of table renaming detection: $renameInfo',
            label: 'MigrationManager._detectRenamedTables',
          );
        }
      }

      if (tablesToCreate.isNotEmpty) {
        final userTablesToCreate = tablesToCreate
            .where((name) => !SystemTable.isSystemTable(name))
            .toList();
        if (userTablesToCreate.isNotEmpty) {
          Logger.info(
            'Tables to be created: ${userTablesToCreate.join(', ')}',
            label: 'MigrationManager._detectRenamedTables',
          );
        }
      }

      if (tablesToDrop.isNotEmpty) {
        final userTablesToDrop = tablesToDrop
            .where((name) => !SystemTable.isSystemTable(name))
            .toList();
        if (userTablesToDrop.isNotEmpty) {
          Logger.info(
            'Tables to be deleted: ${userTablesToDrop.join(', ')}',
            label: 'MigrationManager._detectRenamedTables',
          );
        }
      }

      return RenamedTableResult(
        renamedTables: renamedTables,
        tablesToCreate: tablesToCreate,
        tablesToDrop: tablesToDrop,
      );
    } catch (e, stack) {
      Logger.error(
        'Error occurred during table renaming detection: $e\n$stack',
        label: 'MigrationManager._detectRenamedTables',
      );

      // Return empty result in case of error to avoid misjudgment
      return const RenamedTableResult(
        renamedTables: {},
        tablesToCreate: [],
        tablesToDrop: [],
      );
    }
  }

  /// Detect renamed tables by tableId
  Future<void> _detectRenamedTablesByTableId(
      List<String> existingTables,
      List<TableSchema> newSchemas,
      Map<String, TableSchema> renamedTables) async {
    final matchedExistingTables = <String>{};
    final matchedNewSchemas = <TableSchema>{};

    for (final newSchema in newSchemas) {
      // Only process tables with tableId set
      if (newSchema.tableId == null) continue;

      for (final existingTableName in existingTables) {
        // Skip tables with the same name, these don't need to be identified as renamed
        if (existingTableName == newSchema.name) continue;

        final existingSchema =
            await _dataStore.schemaManager?.getTableSchema(existingTableName);
        if (existingSchema == null) continue;

        // Check if tableId matches
        if (existingSchema.tableId != null &&
            existingSchema.tableId == newSchema.tableId &&
            existingTableName != newSchema.name) {
          // Record renamed table
          renamedTables[existingTableName] = newSchema;
          matchedExistingTables.add(existingTableName);
          matchedNewSchemas.add(newSchema);

          break;
        }
      }
    }

    // Remove matched tables
    existingTables
        .removeWhere((table) => matchedExistingTables.contains(table));
    newSchemas.removeWhere((schema) => matchedNewSchemas.contains(schema));
  }

  /// Detect renamed tables by similarity
  Future<void> _detectRenamedTablesBySimilarity(
      List<String> existingTables,
      List<TableSchema> newSchemas,
      Map<String, TableSchema> renamedTables) async {
    try {
      // Similarity threshold, above which tables are considered the same
      const similarityThreshold = 0.75;

      // prepare parallel calculation requests
      final similarityRequests = <TableSimilarityRequest>[];
      final existingSchemasMap = <String, TableSchema>{};

      // get all old table schemas
      for (final existingTableName in existingTables) {
        final existingSchema =
            await _dataStore.schemaManager?.getTableSchema(existingTableName);
        if (existingSchema == null) continue;
        existingSchemasMap[existingTableName] = existingSchema;
      }

      // build all similarity requests
      for (final existingTableName in existingSchemasMap.keys) {
        final existingSchema = existingSchemasMap[existingTableName]!;

        for (final newSchema in newSchemas) {
          // skip tables with the same name, these should not be identified as renamed tables
          if (existingTableName == newSchema.name) {
            continue;
          }

          similarityRequests.add(TableSimilarityRequest(
            oldSchema: existingSchema,
            newSchema: newSchema,
            oldTableIndex: existingTables.indexOf(existingTableName),
            newTableIndex: newSchemas.indexOf(newSchema),
            oldTablesCount: existingTables.length,
            newTablesCount: newSchemas.length,
          ));
        }
      }

      // if no requests to process, return
      if (similarityRequests.isEmpty) {
        return;
      }

      // max concurrent
      final maxConcurrent = _dataStore.config.maxConcurrency;

      // batch processing requests
      final int batchSize = (similarityRequests.length / maxConcurrent).ceil();
      final batches = <List<TableSimilarityRequest>>[];

      for (int i = 0; i < similarityRequests.length; i += batchSize) {
        final end = min(i + batchSize, similarityRequests.length);
        batches.add(similarityRequests.sublist(i, end));
      }

      // parallel processing all batches
      final batchResults = await Future.wait(batches.map((batch) =>
          ComputeManager.run(calculateBatchTableSimilarity,
              BatchTableSimilarityRequest(requests: batch),
              useIsolate: similarityRequests.length > 20)));

      // merge all results
      final allResults = <TableSimilarityResult>[];
      for (final batchResult in batchResults) {
        allResults.addAll(batchResult.results);
      }

      // sort results by similarity
      allResults.sort((a, b) => b.similarity.compareTo(a.similarity));

      // greedy matching algorithm
      final processedOldTables = <String>{};
      final processedNewSchemas = <TableSchema>{};

      for (final result in allResults) {
        // if table is already processed, skip
        if (processedOldTables.contains(result.oldTableName) ||
            processedNewSchemas.contains(result.newSchema)) {
          continue;
        }

        // if best match is above threshold, consider as renamed table
        if (result.similarity >= similarityThreshold) {
          // additional check: even with high similarity, tables with the same name should not be considered renamed
          if (result.oldTableName == result.newSchema.name) {
            continue;
          }

          // add to renamed results
          renamedTables[result.oldTableName] = result.newSchema;

          // record processed tables
          processedOldTables.add(result.oldTableName);
          processedNewSchemas.add(result.newSchema);

          // remove from remaining lists
          existingTables.remove(result.oldTableName);
          newSchemas.remove(result.newSchema);
        } else {
          // if similarity is not high enough, break
          break;
        }
      }
    } catch (e, stack) {
      Logger.error(
        'Error during parallel table similarity detection: $e\n$stack',
        label: 'MigrationManager._detectRenamedTablesBySimilarity',
      );
    }
  }

  /// Migrate existing table schema
  Future<MigrationTask?> _migrateExistingTable(TableSchema newSchema,
      {required int batchSize}) async {
    final tableName = newSchema.name;
    final oldSchema = await _dataStore.schemaManager?.getTableSchema(tableName);
    if (oldSchema == null) {
      return null;
    }

    // Compare schemas and generate operations
    final operations =
        _compareSchemasAndGenerateOperations(oldSchema, newSchema);

    if (operations.isEmpty) {
      return null;
    } else {
      if (!SystemTable.isSystemTable(tableName)) {
        Logger.info(
          'Found ${operations.length} changes for table $tableName',
          label: 'MigrationManager._migrateExistingTable',
        );
      }
    }

    // Check if there are existing migration tasks for this table
    final hasPendingTask =
        _pendingTasks.any((task) => task.tableName == tableName);

    // Check if there are any unfinished migration tasks marked in global config
    final globalConfig = await _dataStore.getGlobalConfig();
    if (hasPendingTask || (globalConfig?.hasMigrationTask == true)) {
      // If there are pending tasks, check if there are tasks for this table in migration metadata
      final meta = await _getOrLoadMigrationMeta();
      final allTaskFound = <String>[];

      // Find possible tasks for the same table name
      final tasksToRemove = <String>[];
      for (final entry in meta.directoryMapping.idToDir.entries) {
        try {
          final taskId = entry.key;
          final dirIndex = entry.value;
          final taskPath =
              _dataStore.pathManager.getMigrationTaskPath(dirIndex, taskId);

          // Check if file exists
          final fileExists = await _dataStore.storage.existsFile(taskPath);
          if (!fileExists) {
            // File doesn't exist but mapping has it - mark for cleanup
            tasksToRemove.add(taskId);
            continue;
          }

          final content = await _dataStore.storage.readAsString(taskPath);

          if (content != null && content.isNotEmpty) {
            final task = MigrationTask.fromJson(jsonDecode(content));
            if (task.tableName == tableName &&
                task.pendingMigrationSpaces.isNotEmpty) {
              allTaskFound.add(taskId);
            }
          } else {
            // File exists but is empty - mark for cleanup
            tasksToRemove.add(taskId);
          }
        } catch (e) {
          // File reading error - mark for cleanup
          final taskId = entry.key;
          tasksToRemove.add(taskId);
        }
      }

      // Clean up orphaned mappings if any found
      if (tasksToRemove.isNotEmpty) {
        await _cleanupOrphanedMappings(tasksToRemove);
      }

      if (allTaskFound.isNotEmpty) {
        Logger.info(
          'Table [$tableName] already has ${allTaskFound.length} unfinished migration tasks (${allTaskFound.join(',')}), skipping adding new task',
          label: 'MigrationManager._migrateExistingTable',
        );
        return null;
      }
    }

    final task = await addMigrationTask(tableName, operations,
        startProcessing: false, isAutoGenerated: true);
    return task;
  }

  /// Add migration task for table schema update across all spaces
  Future<MigrationTask> addMigrationTask(
      String tableName, List<MigrationOperation> operations,
      {bool isAutoGenerated = false,
      bool startProcessing = true,
      bool allowAfterDataMigration = false}) async {
    try {
      final oldSchema =
          await _dataStore.schemaManager?.getTableSchema(tableName);
      if (oldSchema != null) {
        // Only check for existing tables
        final requiresMigration =
            await _requiresDataMigration(operations, oldSchema);

        if (requiresMigration) {
          bool isAllowed = false;
          if (allowAfterDataMigration) {
            // From SchemaBuilder
            isAllowed = true;
          } else if (isAutoGenerated) {
            // From migrate()
            final allowedTables = _dataStore
                    .config.migrationConfig?.allowedAfterDataMigrationTables ??
                [];
            if (allowedTables.contains(tableName)) {
              isAllowed = true;
            }
          }

          if (!isAllowed) {
            throw Exception(
                'Migration for table "$tableName" requires data modification and was not explicitly allowed. '
                'This is to prevent accidental data loss or long-running migrations. \n'
                'For changes during app startup, add the table name to `MigrationConfig.allowedAfterDataMigrationTables`. \n'
                'For changes via SchemaBuilder, use the `.allowAfterDataMigration()` method before calling `.future`.');
          }
        }
      }
      // create new migration task
      final taskId = DateTime.now().microsecondsSinceEpoch.toString();
      final dirIndex = await _getNextDirIndex();
      final spaces = await _getAllSpaces();

      final task = MigrationTask(
        taskId: taskId,
        tableName: tableName,
        isSchemaUpdated: false,
        pendingMigrationSpaces: spaces,
        operations: operations,
        createTime: DateTime.now(),
        dirIndex: dirIndex,
        isAutoGenerated: isAutoGenerated,
      );

      // persist migration task
      await _saveMigrationTask(task);
      _pendingTasks.add(task);

      // update global config
      final globalConfig = await _dataStore.getGlobalConfig();
      if (globalConfig != null && !globalConfig.hasMigrationTask) {
        // only update when hasMigrationTask is false, avoid unnecessary write
        await _dataStore
            .saveGlobalConfig(globalConfig.copyWith(hasMigrationTask: true));
      }

      // only trigger task processing when startProcessing is true
      if (startProcessing) {
        processMigrationTasks();
      }

      return task;
    } catch (e, stack) {
      Logger.error(
        'Add migration task failed: $e\n$stack',
        label: 'MigrationManager.addMigrationTask',
      );
      rethrow;
    }
  }

  /// Execute operations - only handle table structure updates
  Future<void> executeSchemaOperations(
      String tableName, List<MigrationOperation> operations) async {
    for (var operation in operations) {
      await _executeSchemaOperation(tableName, operation);
    }
  }

  Future<void> _executeSchemaOperation(
      String tableName, MigrationOperation operation) async {
    switch (operation.type) {
      case MigrationType.addField:
        await _dataStore.addField(tableName, operation.field!);
        break;

      case MigrationType.removeField:
        await _dataStore.removeField(tableName, operation.fieldName!);
        break;

      case MigrationType.renameField:
        await _dataStore.renameField(
          tableName,
          operation.fieldName!,
          operation.newName!,
        );
        break;

      case MigrationType.modifyField:
        await _dataStore.modifyField(
          tableName,
          operation.fieldUpdate!.name,
          operation.fieldUpdate!,
        );
        break;

      case MigrationType.addIndex:
        // Index operations are handled in each space's migration instance
        break;

      case MigrationType.removeIndex:
        // Index operations are handled in each space's migration instance
        break;

      case MigrationType.modifyIndex:
        // Index operations are handled in each space's migration instance
        break;

      case MigrationType.renameTable:
        final oldSchema =
            await _dataStore.schemaManager?.getTableSchema(tableName);
        if (oldSchema == null) {
          return;
        }
        final newSchema = oldSchema.copyWith(name: operation.newTableName!);
        await _dataStore.createTable(newSchema);
        break;

      case MigrationType.dropTable:
        break;

      case MigrationType.setPrimaryKeyConfig:
        if (operation.primaryKeyConfig != null) {
          await _dataStore.setPrimaryKeyConfig(
            tableName,
            operation.primaryKeyConfig!,
          );
        }
        break;

      case MigrationType.addForeignKey:
        // Add foreign key to table schema
        final schema =
            await _dataStore.schemaManager?.getTableSchema(tableName);
        if (schema == null) {
          Logger.warn(
            'Table $tableName does not exist, cannot add foreign key',
            label: 'MigrationManager._executeSchemaOperation',
          );
          break;
        }
        // Check if foreign key already exists
        if (schema.foreignKeys
            .any((fk) => fk.actualName == operation.foreignKey!.actualName)) {
          Logger.warn(
            'Foreign key ${operation.foreignKey!.actualName} already exists in table $tableName',
            label: 'MigrationManager._executeSchemaOperation',
          );
          break;
        }
        // Add foreign key to schema
        final newForeignKeys = [...schema.foreignKeys, operation.foreignKey!];
        final updatedSchema = schema.copyWith(foreignKeys: newForeignKeys);
        await _dataStore.schemaManager!
            .saveTableSchema(tableName, updatedSchema);
        // Update system table (updateTableSchema will handle it, but we call explicitly for clarity)
        // Note: updateTableSchema no longer unconditionally calls updateSystemTableForTable
        // We call it explicitly here after schema update
        final fkManager = _dataStore.foreignKeyManager;
        await fkManager?.updateSystemTableForTable(tableName, updatedSchema);
        // Auto-create index if needed
        if (operation.foreignKey!.autoCreateIndex) {
          await fkManager?.createForeignKeyIndexes(tableName);
        }
        break;

      case MigrationType.removeForeignKey:
        // Remove foreign key from table schema
        final schema =
            await _dataStore.schemaManager?.getTableSchema(tableName);
        if (schema == null) {
          Logger.warn(
            'Table $tableName does not exist, cannot remove foreign key',
            label: 'MigrationManager._executeSchemaOperation',
          );
          break;
        }

        // Find the foreign key to be removed
        final fkToRemove = schema.foreignKeys.firstWhere(
          (fk) => fk.actualName == operation.foreignKeyName,
          orElse: () => throw ArgumentError(
            'Foreign key ${operation.foreignKeyName} not found in table $tableName',
          ),
        );

        // Check for orphaned records (records that reference non-existent records)
        // This is important for data integrity - warn developer about potential issues
        await _checkOrphanedRecordsBeforeRemovingForeignKey(
            tableName, fkToRemove);

        // Remove foreign key from schema
        final newForeignKeys = schema.foreignKeys
            .where((fk) => fk.actualName != operation.foreignKeyName)
            .toList();
        final updatedSchema = schema.copyWith(foreignKeys: newForeignKeys);
        await _dataStore.schemaManager!
            .saveTableSchema(tableName, updatedSchema);
        // Update system table
        final fkManager = _dataStore.foreignKeyManager;
        await fkManager?.updateSystemTableForTable(tableName, updatedSchema);
        break;

      case MigrationType.modifyForeignKey:
        // Modify foreign key in table schema
        final schema =
            await _dataStore.schemaManager?.getTableSchema(tableName);
        if (schema == null) {
          Logger.warn(
            'Table $tableName does not exist, cannot modify foreign key',
            label: 'MigrationManager._executeSchemaOperation',
          );
          break;
        }
        // Find the foreign key to modify
        final fkName =
            operation.foreignKeyName ?? operation.foreignKey?.actualName;
        if (fkName == null) {
          Logger.warn(
            'Foreign key name not provided for modify operation',
            label: 'MigrationManager._executeSchemaOperation',
          );
          break;
        }

        final oldFk = schema.foreignKeys.firstWhere(
          (fk) => fk.actualName == fkName,
          orElse: () => throw ArgumentError(
            'Foreign key $fkName not found in table $tableName',
          ),
        );

        // Merge old FK with new properties from operation.foreignKey
        // Only non-core properties (onDelete, onUpdate, enabled, autoCreateIndex, comment) are modified
        // Core properties (fields, referencedTable, referencedFields) are preserved from old FK
        final newFk = oldFk.copyWith(
          onDelete: operation.foreignKey?.onDelete ?? oldFk.onDelete,
          onUpdate: operation.foreignKey?.onUpdate ?? oldFk.onUpdate,
          enabled: operation.foreignKey?.enabled ?? oldFk.enabled,
          autoCreateIndex:
              operation.foreignKey?.autoCreateIndex ?? oldFk.autoCreateIndex,
          comment: operation.foreignKey?.comment ?? oldFk.comment,
        );

        // Find and replace foreign key
        final newForeignKeys = schema.foreignKeys.map((fk) {
          if (fk.actualName == fkName) {
            return newFk;
          }
          return fk;
        }).toList();
        final updatedSchema = schema.copyWith(foreignKeys: newForeignKeys);
        await _dataStore.schemaManager!
            .saveTableSchema(tableName, updatedSchema);
        // Update system table
        final fkManager = _dataStore.foreignKeyManager;
        await fkManager?.updateSystemTableForTable(tableName, updatedSchema);
        break;
    }
  }

  /// Compare schemas and generate operations
  List<MigrationOperation> _compareSchemasAndGenerateOperations(
    TableSchema oldSchema,
    TableSchema newSchema,
  ) {
    if (oldSchema.isGlobal != newSchema.isGlobal) {
      throw Exception(
          'Changing the "isGlobal" property for an existing table (${newSchema.name}) is not supported. This operation requires complex data migration between spaces and the global scope, which must be handled manually.');
    }
    final operations = <MigrationOperation>[];

    // Check field changes
    _compareFields(oldSchema, newSchema, operations);

    // check primary key config change
    if (_isPrimaryKeyConfigChanged(oldSchema, newSchema)) {
      operations.add(MigrationOperation(
        type: MigrationType.setPrimaryKeyConfig,
        primaryKeyConfig: newSchema.primaryKeyConfig,
        oldPrimaryKeyConfig: oldSchema.primaryKeyConfig,
      ));
    }

    // Check indexes changes
    _compareIndexes(oldSchema, newSchema, operations);

    // Check foreign key changes
    _compareForeignKeys(oldSchema, newSchema, operations);

    return operations;
  }

  /// check primary key config change (compare detailed config instead of only type)
  bool _isPrimaryKeyConfigChanged(
      TableSchema oldSchema, TableSchema newSchema) {
    // check added properties change
    // 1. check primary key field name change
    if (oldSchema.primaryKeyConfig.name != newSchema.primaryKeyConfig.name) {
      Logger.info(
        'Primary key name change detected: ${oldSchema.primaryKeyConfig.name} -> ${newSchema.primaryKeyConfig.name}',
        label: 'MigrationManager._isPrimaryKeyConfigChanged',
      );
      return true;
    }

    // 2. check primary key ordered change
    if (oldSchema.primaryKeyConfig.isOrdered !=
        newSchema.primaryKeyConfig.isOrdered) {
      Logger.info(
        'Primary key ordering change detected: ${oldSchema.primaryKeyConfig.isOrdered} -> ${newSchema.primaryKeyConfig.isOrdered}',
        label: 'MigrationManager._isPrimaryKeyConfigChanged',
      );
      return true;
    }

    // check primary key generation type change
    if (oldSchema.primaryKeyConfig.type != newSchema.primaryKeyConfig.type) {
      return true;
    }

    // if both are sequential increment mode, check detailed config
    if (oldSchema.primaryKeyConfig.type == PrimaryKeyType.sequential &&
        newSchema.primaryKeyConfig.type == PrimaryKeyType.sequential) {
      final oldConfig = oldSchema.primaryKeyConfig.sequentialConfig;
      final newConfig = newSchema.primaryKeyConfig.sequentialConfig;

      // special handling: if one is null but the other is not null
      if ((oldConfig == null) != (newConfig == null)) {
        // if old config is default config and new config is null, consider as same
        if (newConfig == null &&
            oldConfig != null &&
            oldConfig.initialValue == 1 &&
            oldConfig.increment == 1 &&
            !oldConfig.useRandomIncrement) {
          return false; // consider as no change
        }
        // if new config is default config and old config is null, consider as same
        if (oldConfig == null &&
            newConfig != null &&
            newConfig.initialValue == 1 &&
            newConfig.increment == 1 &&
            !newConfig.useRandomIncrement) {
          return false; // consider as no change
        }
        return true;
      }

      // if both are not null, compare detailed config
      if (oldConfig != null && newConfig != null) {
        if (oldConfig.initialValue != newConfig.initialValue ||
            oldConfig.increment != newConfig.increment ||
            oldConfig.useRandomIncrement != newConfig.useRandomIncrement) {
          return true;
        }
      }
    }

    // if both are timestamp based type, no need to compare detailed config
    if ((oldSchema.primaryKeyConfig.type == PrimaryKeyType.timestampBased ||
                oldSchema.primaryKeyConfig.type ==
                    PrimaryKeyType.datePrefixed) &&
            (newSchema.primaryKeyConfig.type == PrimaryKeyType.timestampBased ||
                newSchema.primaryKeyConfig.type ==
                    PrimaryKeyType.datePrefixed) ||
        (oldSchema.primaryKeyConfig.type == PrimaryKeyType.shortCode ||
            newSchema.primaryKeyConfig.type == PrimaryKeyType.shortCode)) {
      // if timestamp based type changed to date prefixed type, or vice versa, consider as change
      return oldSchema.primaryKeyConfig.type != newSchema.primaryKeyConfig.type;
    }

    // all conditions are not met, config is not changed
    return false;
  }

  /// Compare fields and generate operations
  void _compareFields(
    TableSchema oldSchema,
    TableSchema newSchema,
    List<MigrationOperation> operations,
  ) {
    // check added fields
    for (var newField in newSchema.fields) {
      if (!oldSchema.fields.any((f) => f.name == newField.name)) {
        operations.add(MigrationOperation(
          type: MigrationType.addField,
          field: newField,
        ));
        continue;
      }

      // check modified fields
      final oldField = oldSchema.fields.firstWhere(
        (f) => f.name == newField.name,
      );
      if (_isFieldModified(oldField, newField)) {
        // Add check for dangerous type conversions before adding the operation
        if (oldField.type != newField.type) {
          _preventDangerousTypeConversion(oldField, newField);
        }

        if (!SystemTable.isSystemTable(newSchema.name)) {
          Logger.info(
            'Table ${newSchema.name}, field ${newField.name} has been modified',
            label: 'MigrationManager._compareSchemas',
          );
        }
        operations.add(MigrationOperation(
          type: MigrationType.modifyField,
          fieldUpdate: FieldSchemaUpdate(
            name: newField.name,
            type: newField.type,
            nullable: newField.nullable,
            defaultValue: newField.defaultValue,
            unique: newField.unique,
            comment: newField.comment,
            minLength: newField.minLength,
            maxLength: newField.maxLength,
            minValue: newField.minValue,
            maxValue: newField.maxValue,
            defaultValueType: newField.defaultValueType,
          ),
        ));
      }
    }

    // Check for removed fields
    for (var oldField in oldSchema.fields) {
      if (!newSchema.fields.any((f) => f.name == oldField.name)) {
        operations.add(MigrationOperation(
          type: MigrationType.removeField,
          fieldName: oldField.name,
        ));
      }
    }

    // Check for renamed fields
    _detectRenamedFields(oldSchema, newSchema, operations);
  }

  /// Compare indexes and generate operations
  void _compareIndexes(
    TableSchema oldSchema,
    TableSchema newSchema,
    List<MigrationOperation> operations,
  ) {
    // First mark all old indexes as to be removed
    final indexesToRemove = List<IndexSchema>.from(oldSchema.indexes);

    // Check for added and modified indexes
    for (var newIndex in newSchema.indexes) {
      // Try to find matching index in old schema
      IndexSchema? matchedOldIndex;

      for (var oldIndex in oldSchema.indexes) {
        if (_areIndexesSame(oldIndex, newIndex)) {
          matchedOldIndex = oldIndex;
          break;
        }
      }

      if (matchedOldIndex == null) {
        // No matching old index found, this is a new index
        operations.add(MigrationOperation(
          type: MigrationType.addIndex,
          index: newIndex,
        ));
      } else {
        // Found matching old index, remove from to-be-deleted list
        indexesToRemove.remove(matchedOldIndex);

        // Check if modification is needed
        if (_isIndexModified(matchedOldIndex, newIndex)) {
          operations.add(MigrationOperation(
            type: MigrationType.modifyIndex,
            indexName: newIndex.indexName,
            fields: newIndex.fields,
            unique: newIndex.unique,
          ));
        }
      }
    }

    // Handle indexes that need to be removed
    for (var indexToRemove in indexesToRemove) {
      if (!SystemTable.isSystemTable(oldSchema.name)) {
        Logger.info(
          'Detected index to be removed: ${indexToRemove.actualIndexName}, fields: ${indexToRemove.fields.join(", ")}',
          label: 'MigrationManager._compareIndexes',
        );
      }

      operations.add(MigrationOperation(
        type: MigrationType.removeIndex,
        indexName: indexToRemove
            .actualIndexName, // Use actualIndexName instead of indexName
        fields: indexToRemove
            .fields, // Also provide field list for more reliable matching
      ));
    }
  }

  /// Check if two indexes are the same
  bool _areIndexesSame(IndexSchema a, IndexSchema b) {
    if (a.indexName != null && b.indexName != null) {
      return a.indexName == b.indexName;
    }
    // if no indexName, compare field list
    return _areFieldListsEqual(a.fields, b.fields);
  }

  /// Compare two field lists ignoring order
  bool _areFieldListsEqual(List<String> a, List<String> b) {
    if (a.length != b.length) return false;
    final setA = Set<String>.from(a);
    final setB = Set<String>.from(b);
    return setA.difference(setB).isEmpty;
  }

  /// Check if index is modified
  bool _isIndexModified(IndexSchema oldIndex, IndexSchema newIndex) {
    if (oldIndex.indexName != null && newIndex.indexName != null) {
      if (oldIndex.indexName != newIndex.indexName) {
        return true;
      }
    }
    return oldIndex.unique != newIndex.unique ||
        !_areFieldListsEqual(oldIndex.fields, newIndex.fields);
  }

  /// Compare foreign keys and generate operations
  ///
  /// Foreign key change rules:
  /// - **Allowed to modify**: onDelete, onUpdate, enabled, autoCreateIndex, comment
  /// - **Not allowed to modify**: fields, referencedTable, referencedFields
  ///   (These are core definitions. If changed, must remove old FK and add new FK)
  void _compareForeignKeys(
    TableSchema oldSchema,
    TableSchema newSchema,
    List<MigrationOperation> operations,
  ) {
    // First mark all old foreign keys as to be removed
    final foreignKeysToRemove =
        List<ForeignKeySchema>.from(oldSchema.foreignKeys);

    // Check for added and modified foreign keys
    for (var newFk in newSchema.foreignKeys) {
      // Try to find matching foreign key in old schema by name
      ForeignKeySchema? matchedOldFk;

      for (var oldFk in oldSchema.foreignKeys) {
        // Match by actual name (handles auto-generated names)
        if (oldFk.actualName == newFk.actualName) {
          matchedOldFk = oldFk;
          break;
        }
      }

      if (matchedOldFk == null) {
        // No matching old foreign key found, this is a new foreign key
        operations.add(MigrationOperation(
          type: MigrationType.addForeignKey,
          foreignKey: newFk,
        ));
      } else {
        // Found matching old foreign key, remove from to-be-deleted list
        foreignKeysToRemove.remove(matchedOldFk);

        // Check if modification is needed
        // Core definitions (fields, referencedTable, referencedFields) cannot be modified
        // If they change, we must remove old FK and add new FK
        final coreDefinitionChanged =
            !_areFieldListsEqual(matchedOldFk.fields, newFk.fields) ||
                matchedOldFk.referencedTable != newFk.referencedTable ||
                !_areFieldListsEqual(
                    matchedOldFk.referencedFields, newFk.referencedFields);

        if (coreDefinitionChanged) {
          // Core definition changed - this is a breaking change that requires manual handling
          // Throwing exception to warn developer that this requires data migration
          throw Exception(
            'Foreign key core definition change detected for ${matchedOldFk.actualName} in table ${oldSchema.name}. '
            'Core definitions (fields, referencedTable, referencedFields) cannot be automatically modified. '
            'This is a breaking change that may cause data inconsistency.\n'
            'Old definition: fields=${matchedOldFk.fields}, referencedTable=${matchedOldFk.referencedTable}, referencedFields=${matchedOldFk.referencedFields}\n'
            'New definition: fields=${newFk.fields}, referencedTable=${newFk.referencedTable}, referencedFields=${newFk.referencedFields}\n'
            'Please handle this manually:\n'
            '1. Remove the old foreign key: db.schema("${oldSchema.name}").removeForeignKey("${matchedOldFk.actualName}")\n'
            '2. Ensure data integrity (check for orphaned records, update data if needed)\n'
            '3. Add the new foreign key: db.schema("${oldSchema.name}").addForeignKey(...)',
          );
        } else {
          // Only non-core properties changed - can modify
          final needsModification = matchedOldFk.onDelete != newFk.onDelete ||
              matchedOldFk.onUpdate != newFk.onUpdate ||
              matchedOldFk.enabled != newFk.enabled ||
              matchedOldFk.autoCreateIndex != newFk.autoCreateIndex ||
              matchedOldFk.comment != newFk.comment;

          if (needsModification) {
            operations.add(MigrationOperation(
              type: MigrationType.modifyForeignKey,
              foreignKey: newFk,
              oldForeignKey: matchedOldFk,
            ));
          }
        }
      }
    }

    // Handle foreign keys that need to be removed
    for (var fkToRemove in foreignKeysToRemove) {
      if (!SystemTable.isSystemTable(oldSchema.name)) {
        Logger.info(
          'Detected foreign key to be removed: ${fkToRemove.actualName}',
          label: 'MigrationManager._compareForeignKeys',
        );
      }

      operations.add(MigrationOperation(
        type: MigrationType.removeForeignKey,
        foreignKeyName: fkToRemove.actualName,
      ));
    }
  }

  /// Detect renamed fields using strict matching
  void _detectRenamedFields(
    TableSchema oldSchema,
    TableSchema newSchema,
    List<MigrationOperation> operations,
  ) {
    // Get removed and added fields
    final removedFields = operations
        .where((op) => op.type == MigrationType.removeField)
        .map((op) => op.fieldName!)
        .toList();
    final addedFields = operations
        .where((op) => op.type == MigrationType.addField)
        .map((op) => op.field!)
        .toList();

    // if no removed or added fields, no need to detect renamed fields
    if (removedFields.isEmpty || addedFields.isEmpty) return;

    // match renamed fields by fieldId directly
    _detectRenamedFieldsByFieldId(
        oldSchema, newSchema, operations, removedFields, addedFields);

    // if there are still fields to match, match by similarity
    if (removedFields.isNotEmpty && addedFields.isNotEmpty) {
      _detectRenamedFieldsBySimilarityParallel(
          oldSchema, newSchema, operations, removedFields, addedFields);
    }
  }

  /// match renamed fields by fieldId directly
  void _detectRenamedFieldsByFieldId(
    TableSchema oldSchema,
    TableSchema newSchema,
    List<MigrationOperation> operations,
    List<String> removedFields,
    List<FieldSchema> addedFields,
  ) {
    // for storing matched fields, avoid duplicate processing
    final matchedRemovedFields = <String>{};
    final matchedAddedFields = <FieldSchema>{};

    // iterate added fields
    for (var newField in addedFields) {
      // only process fields with fieldId
      if (newField.fieldId == null) continue;

      // if new field is primary key, skip
      if (newField.name == newSchema.primaryKey) continue;

      // find field in old schema with same fieldId
      for (var oldFieldName in removedFields) {
        // if old field is primary key, skip
        if (oldSchema.primaryKey == oldFieldName) continue;

        final oldField = oldSchema.fields.firstWhere(
          (f) => f.name == oldFieldName,
          orElse: () => const FieldSchema(name: '', type: DataType.text),
        );

        // if found matching fieldId, consider as renamed
        if (oldField.fieldId != null && oldField.fieldId == newField.fieldId) {
          // remove existing add and remove operations
          operations.removeWhere((op) =>
              (op.type == MigrationType.removeField &&
                  op.fieldName == oldFieldName) ||
              (op.type == MigrationType.addField && op.field == newField));

          // add rename operation
          operations.add(MigrationOperation(
            type: MigrationType.renameField,
            fieldName: oldFieldName,
            newName: newField.name,
          ));

          // record matched fields
          matchedRemovedFields.add(oldFieldName);
          matchedAddedFields.add(newField);
          break;
        }
      }
    }

    // remove matched fields from original list
    removedFields.removeWhere((field) => matchedRemovedFields.contains(field));
    addedFields.removeWhere((field) => matchedAddedFields.contains(field));
  }

  /// parallel way to detect renamed fields
  Future<void> _detectRenamedFieldsBySimilarityParallel(
    TableSchema oldSchema,
    TableSchema newSchema,
    List<MigrationOperation> operations,
    List<String> removedFields,
    List<FieldSchema> addedFields,
  ) async {
    // field similarity threshold, only fields with score above this value will be considered as renamed
    const similarityThreshold = 0.6;

    // prepare parallel calculation requests
    final similarityRequests = <FieldSimilarityRequest>[];

    // iterate all fields to compare
    for (var oldFieldName in removedFields) {
      // if old field is primary key, skip
      if (oldSchema.primaryKey == oldFieldName) continue;

      final oldField = oldSchema.fields.firstWhere(
        (f) => f.name == oldFieldName,
      );

      for (var newField in addedFields) {
        // if new field is primary key, skip
        if (newSchema.primaryKey == newField.name) continue;

        similarityRequests.add(FieldSimilarityRequest(
          oldField: oldField,
          newField: newField,
          oldFieldIndex: oldSchema.fields.indexOf(oldField),
          newFieldIndex: newSchema.fields.indexOf(newField),
          oldFieldsCount: oldSchema.fields.length,
          newFieldsCount: newSchema.fields.length,
          oldSchema: oldSchema,
          newSchema: newSchema,
        ));
      }
    }

    // if no requests to process, return
    if (similarityRequests.isEmpty) {
      return;
    }

    // max concurrent
    final maxConcurrent = _dataStore.config.maxConcurrency;

    // batch processing requests
    final int batchSize = (similarityRequests.length / maxConcurrent).ceil();
    final batches = <List<FieldSimilarityRequest>>[];

    for (int i = 0; i < similarityRequests.length; i += batchSize) {
      final end = min(i + batchSize, similarityRequests.length);
      batches.add(similarityRequests.sublist(i, end));
    }

    // parallel processing all batches
    final batchResults = await Future.wait(batches.map((batch) =>
        ComputeManager.run(calculateBatchFieldSimilarity,
            BatchFieldSimilarityRequest(requests: batch),
            useIsolate: similarityRequests.length > 100)));

    // merge all results
    final allResults = <FieldSimilarityResult>[];
    for (final batchResult in batchResults) {
      allResults.addAll(batchResult.results);
    }

    // sort results by similarity
    allResults.sort((a, b) => b.similarity.compareTo(a.similarity));

    // greedy matching algorithm
    final processedOldFields = <String>{};
    final processedNewFields = <FieldSchema>{};

    for (final result in allResults) {
      // if field is already processed, skip
      if (processedOldFields.contains(result.oldFieldName) ||
          processedNewFields.contains(result.newField)) {
        continue;
      }

      // if best match is above threshold, consider as renamed field
      if (result.similarity >= similarityThreshold) {
        // remove existing add and remove operations
        operations.removeWhere((op) =>
            (op.type == MigrationType.removeField &&
                op.fieldName == result.oldFieldName) ||
            (op.type == MigrationType.addField && op.field == result.newField));

        // add rename operation
        operations.add(MigrationOperation(
          type: MigrationType.renameField,
          fieldName: result.oldFieldName,
          newName: result.newField.name,
        ));

        // record processed fields
        processedOldFields.add(result.oldFieldName);
        processedNewFields.add(result.newField);

        // remove from processing list
        removedFields.remove(result.oldFieldName);
        addedFields.remove(result.newField);
      } else {
        // if similarity is not high enough, break
        break;
      }
    }
  }

  /// Prevents unsafe data type conversions during schema migration.
  void _preventDangerousTypeConversion(
      FieldSchema oldField, FieldSchema newField) {
    final oldType = oldField.type;
    final newType = newField.type;
    bool isDangerous = false;
    String reason = '';

    // General rule: Converting from a less restrictive type to a more restrictive one is dangerous.
    // e.g., Text -> Integer, Blob -> Anything, Vector -> Anything (except itself)

    if (newType == DataType.integer ||
        newType == DataType.double ||
        newType == DataType.bigInt ||
        newType == DataType.datetime) {
      if (oldType == DataType.text ||
          oldType == DataType.blob ||
          oldType == DataType.vector ||
          oldType == DataType.json ||
          oldType == DataType.array) {
        isDangerous = true;
        reason = 'cannot be reliably converted to a numeric or date type.';
      }
    }

    // Changing from a vector to anything else is dangerous because embeddings would be lost.
    if (oldType == DataType.vector && newType != DataType.vector) {
      isDangerous = true;
      reason = 'would discard all existing vector embedding data.';
    }

    // Changing from blob, json, or array to an incompatible type is dangerous.
    if ((oldType == DataType.blob ||
            oldType == DataType.json ||
            oldType == DataType.array) &&
        (newType != oldType && newType != DataType.text)) {
      isDangerous = true;
      reason =
          'is a complex type and cannot be safely converted to the target type.';
    }

    if (isDangerous) {
      throw Exception(
          'Unsupported data type change for field "${newField.name}" from ${oldType.name} to ${newType.name}. This conversion is unsafe because existing data $reason This could lead to data loss or migration failure. Please handle this migration manually by creating a new field and migrating the data yourself.');
    }
  }

  /// Check if field is modified
  bool _isFieldModified(FieldSchema oldField, FieldSchema newField) {
    // check if default value is equal, special handling for datetime type
    bool areDefaultValuesEqual() {
      // if two default values are fully equal, return true directly
      if (oldField.defaultValue == newField.defaultValue) {
        return true;
      }

      // if datetime type, ignore default value change
      if (oldField.type == DataType.datetime &&
          newField.type == DataType.datetime) {
        return true; // consider as equal, no field change
      }

      // other types, default value different consider as not equal
      return false;
    }

    // Check for vector config changes, as this would invalidate existing vector data.
    if (jsonEncode(oldField.vectorConfig?.toJson()) !=
        jsonEncode(newField.vectorConfig?.toJson())) {
      Logger.warn(
        'Detected a change in vectorConfig for field "${newField.name}". This is considered a breaking change.',
        label: 'MigrationManager._isFieldModified',
      );
      return true;
    }

    // check if field is modified
    return oldField.type != newField.type ||
        oldField.nullable != newField.nullable ||
        oldField.unique != newField.unique ||
        oldField.maxLength != newField.maxLength ||
        oldField.minLength != newField.minLength ||
        oldField.minValue != newField.minValue ||
        oldField.maxValue != newField.maxValue ||
        !areDefaultValuesEqual() ||
        oldField.comment != newField.comment ||
        oldField.defaultValueType != newField.defaultValueType;
  }

  /// Get next directory index for migration tasks
  Future<int> _getNextDirIndex() async {
    final meta = await _getOrLoadMigrationMeta();
    final mapping = meta.directoryMapping;
    final maxEntriesPerDir = _dataStore.maxEntriesPerDir;

    // Use cached currentDirIndex if available, otherwise calculate
    int currentDirIndex;
    if (_currentDirIndexCache != null) {
      currentDirIndex = _currentDirIndexCache!;
    } else {
      // Find current directory index (highest dirIndex with files)
      currentDirIndex = 0;
      if (mapping.dirToFileCount.isNotEmpty) {
        currentDirIndex =
            mapping.dirToFileCount.keys.reduce((a, b) => a > b ? a : b);
      }
      _currentDirIndexCache = currentDirIndex;
    }

    // Check if current directory is full
    final currentCount = mapping.getFileCount(currentDirIndex);
    if (currentCount >= maxEntriesPerDir) {
      // Allocate new directory
      final newDirIndex = currentDirIndex + 1;
      _currentDirIndexCache = newDirIndex;
      return newDirIndex;
    }

    return currentDirIndex;
  }

  /// Get or load migration metadata from cache or file
  Future<MigrationMeta> _getOrLoadMigrationMeta() async {
    // Return cached value if available
    if (_migrationMetaCache != null) {
      return _migrationMetaCache!;
    }

    // Load from file and cache
    return await _loadMigrationMeta();
  }

  /// Load migration metadata from file and update cache
  Future<MigrationMeta> _loadMigrationMeta() async {
    // If already loading, wait for that operation
    if (_loadingFuture != null) {
      return await _loadingFuture!;
    }

    // Start loading operation
    final loadOp = _performLoadMigrationMeta();
    _loadingFuture = loadOp;

    try {
      final result = await loadOp;
      return result;
    } finally {
      // Clear loading future after completion
      if (identical(_loadingFuture, loadOp)) {
        _loadingFuture = null;
      }
    }
  }

  /// Perform actual loading of migration metadata
  Future<MigrationMeta> _performLoadMigrationMeta() async {
    // Double-check after acquiring lock
    if (_migrationMetaCache != null) {
      return _migrationMetaCache!;
    }

    try {
      final metaPath = _dataStore.pathManager.getMigrationMetaPath();
      final metaContent = await _dataStore.storage.readAsString(metaPath);
      if (metaContent != null && metaContent.isNotEmpty) {
        _migrationMetaCache = MigrationMeta.fromJson(jsonDecode(metaContent));
      } else {
        _migrationMetaCache = MigrationMeta.initial();
      }
    } catch (e) {
      Logger.warn(
        'Load migration meta failed, use initial: $e',
        label: 'MigrationManager._loadMigrationMeta',
      );
      _migrationMetaCache = MigrationMeta.initial();
    }

    // Update currentDirIndex cache
    _updateCurrentDirIndexCache();

    return _migrationMetaCache!;
  }

  /// Update current directory index cache from directoryMapping
  void _updateCurrentDirIndexCache() {
    if (_migrationMetaCache == null) {
      _currentDirIndexCache = null;
      return;
    }

    final mapping = _migrationMetaCache!.directoryMapping;
    if (mapping.dirToFileCount.isNotEmpty) {
      _currentDirIndexCache =
          mapping.dirToFileCount.keys.reduce((a, b) => a > b ? a : b);
    } else {
      _currentDirIndexCache = 0;
    }
  }

  /// Save migration metadata to file and update cache
  Future<void> _saveMigrationMeta(MigrationMeta meta) async {
    try {
      final metaPath = _dataStore.pathManager.getMigrationMetaPath();
      await _dataStore.storage
          .writeAsString(metaPath, jsonEncode(meta.toJson()));

      // Update cache after successful save
      _migrationMetaCache = meta;
      _updateCurrentDirIndexCache();
    } catch (e) {
      Logger.warn(
        'Save migration meta failed: $e',
        label: 'MigrationManager._saveMigrationMeta',
      );
    }
  }

  /// Save migration task to file
  Future<void> _saveMigrationTask(MigrationTask task) async {
    final taskPath =
        _dataStore.pathManager.getMigrationTaskPath(task.dirIndex, task.taskId);
    await _dataStore.storage.writeAsString(taskPath, jsonEncode(task.toJson()));

    // update meta data with directory mapping
    final meta = await _getOrLoadMigrationMeta();
    final currentMapping = meta.directoryMapping;

    // Check if task already exists in mapping
    final existingDirIndex = currentMapping.getDirIndex(task.taskId);

    // Build updated mapping
    final newIdToDir = Map<String, int>.from(currentMapping.idToDir);
    newIdToDir[task.taskId] = task.dirIndex;

    final newDirToFileCount = Map<int, int>.from(currentMapping.dirToFileCount);

    // If task was moved from another directory, decrement old directory count
    if (existingDirIndex != null && existingDirIndex != task.dirIndex) {
      final oldCount = newDirToFileCount[existingDirIndex] ?? 0;
      if (oldCount > 1) {
        newDirToFileCount[existingDirIndex] = oldCount - 1;
      } else {
        // Remove directory from mapping when count reaches 0
        newDirToFileCount.remove(existingDirIndex);
      }
    }

    // Increment new directory count
    final newCount = newDirToFileCount[task.dirIndex] ?? 0;
    newDirToFileCount[task.dirIndex] = newCount + 1;

    final updatedMapping = DirectoryMappingString(
      idToDir: newIdToDir,
      dirToFileCount: newDirToFileCount,
    );

    final updatedMeta = meta.copyWith(directoryMapping: updatedMapping);
    await _saveMigrationMeta(updatedMeta);
  }

  /// Get all space names
  Future<List<String>> _getAllSpaces() async {
    final config = await _dataStore.getGlobalConfig();
    return config?.spaceNames.toList() ?? ['default'];
  }

  /// Process pending migration tasks
  /// [return] boolean value indicating if all tasks are successfully processed
  Future<bool> processMigrationTasks() async {
    if (_isProcessingTasks || _pendingTasks.isEmpty) {
      return true; // no task to process, consider as success
    }

    _isProcessingTasks = true;
    bool success = true; // track if all tasks are successful
    try {
      while (_pendingTasks.isNotEmpty) {
        final task = _pendingTasks.first;

        // set timeout protection, avoid task execution time too long
        const taskTimeout = Duration(minutes: 5); // 5 minutes timeout
        try {
          await _executeMigrationTask(task).timeout(taskTimeout, onTimeout: () {
            Logger.error(
              'Task execution timed out: taskId=${task.taskId}, tableName=${task.tableName}',
              label: 'MigrationManager.processMigrationTasks',
            );

            // timeout consider as failed
            success = false;
            // timeout not remove task, continue to execute next time
            return;
          });

          // task completed successfully, remove and clean up
          _pendingTasks.removeAt(0);
          await _cleanupTask(task);
        } catch (e, stack) {
          Logger.error(
            'Migration task execution failed: $e\n$stack',
            label: 'MigrationManager.processMigrationTasks',
          );

          // task execution failed
          success = false;

          // remove failed task, avoid infinite loop
          if (_pendingTasks.isNotEmpty &&
              _pendingTasks.first.taskId == task.taskId) {
            _pendingTasks.removeAt(0);
          }
        }
      }

      // update global config after all tasks are completed
      final globalConfig = await _dataStore.getGlobalConfig();
      if (globalConfig != null) {
        await _dataStore
            .saveGlobalConfig(globalConfig.copyWith(hasMigrationTask: false));
      }
    } catch (e, stack) {
      Logger.error(
        'Process migration tasks failed: $e\n$stack',
        label: 'MigrationManager.processMigrationTasks',
      );
      success = false;
    } finally {
      _isProcessingTasks = false;
    }

    return success;
  }

  /// Execute single migration task across spaces
  Future<void> _executeMigrationTask(MigrationTask task) async {
    try {
      // record task start time
      final taskStopwatch = Stopwatch()..start();
      if (!SystemTable.isSystemTable(task.tableName)) {
        Logger.info(
          'Starting migration task execution: ${task.taskId}, table: ${task.tableName}',
          label: 'MigrationManager._executeMigrationTask',
        );
      }

      // sort operations, ensure rename field operation is executed after property modification operation
      final sortedOperations = _sortOperations(List.from(task.operations));

      // Save all pending data and runtime metadata before migration to ensure consistency.
      await _dataStore.saveAllCacheBeforeExit();

      // clear table record cache
      // Don't mark as fully cached since migration preserves data, just invalidates cache
      await _dataStore.cacheManager
          .invalidateCache(task.tableName, markAsFullyCached: false);

      // update global table structure first
      if (!task.isSchemaUpdated) {
        await executeSchemaOperations(task.tableName, sortedOperations);
        final updatedTask = task.setSchemaUpdated(true);
        await _saveMigrationTask(updatedTask);
      }

      // get latest space priority order
      final globalConfig = await _dataStore.getGlobalConfig();
      final prioritizedSpaces = globalConfig?.spaceNames.toList() ?? [];

      // sort pending spaces by priority order
      final pendingSpaces = List<String>.from(task.pendingMigrationSpaces);
      pendingSpaces.sort((a, b) {
        final indexA = prioritizedSpaces.indexOf(a);
        final indexB = prioritizedSpaces.indexOf(b);
        return indexA.compareTo(indexB);
      });

      // get updated table name
      String originalTableName = task.tableName;
      String currentTableName = task.tableName;
      MigrationOperation? renameOp;
      try {
        renameOp = task.operations.firstWhere(
          (op) => op.type == MigrationType.renameTable,
        );
      } catch (e) {
        renameOp = null;
      }
      if (renameOp != null && renameOp.newTableName != null) {
        currentTableName = renameOp.newTableName!;
      }

      // get old table structure information
      final oldSchema =
          await _dataStore.schemaManager?.getTableSchema(originalTableName);

      Logger.info(
        'Preparing to migrate data for ${pendingSpaces.length} spaces',
        label: 'MigrationManager._executeMigrationTask',
      );

      // process data migration for each space
      for (var space in pendingSpaces) {
        final spaceStopwatch = Stopwatch()..start();

        // create migration instance for specific space
        final migrationInstance = DataStoreImpl(
            dbPath: _dataStore.config.dbPath,
            dbName: _dataStore.config.dbName,
            config: _dataStore.config.copyWith(spaceName: space),
            isMigrationInstance: true);
        await migrationInstance.initialize();

        // check if need data migration
        bool needDataMigration = false;

        for (var operation in task.operations) {
          if (operation.type == MigrationType.addField ||
              operation.type == MigrationType.modifyField ||
              operation.type == MigrationType.renameField ||
              operation.type == MigrationType.removeField) {
            needDataMigration = true;
          }
          if (operation.type == MigrationType.addIndex) {
            await migrationInstance.indexManager
                ?.addIndex(currentTableName, operation.index!);
          } else if (operation.type == MigrationType.removeIndex) {
            // handle removeIndex operation, support delete by field list or index name
            final indexNameToRemove = operation.indexName;
            final fieldsToRemove = operation.fields;

            if (indexNameToRemove != null) {
              // if index name exists, delete by name
              await migrationInstance.indexManager
                  ?.removeIndex(currentTableName, indexName: indexNameToRemove);
            } else if (fieldsToRemove != null && fieldsToRemove.isNotEmpty) {
              // delete by field list
              await migrationInstance.indexManager
                  ?.removeIndex(currentTableName, fields: fieldsToRemove);
            }
          } else if (operation.type == MigrationType.modifyIndex) {
            await migrationInstance.indexManager?.modifyIndex(
              currentTableName,
              operation.indexName ?? operation.fields!.join('_'),
              IndexSchema(
                indexName: operation.indexName,
                fields: operation.fields!,
                unique: operation.unique ?? false,
              ),
            );
          } else if (operation.type == MigrationType.dropTable) {
            await migrationInstance.dropTable(task.tableName,
                isMigration: true);
          } else if (operation.type == MigrationType.removeField) {
            // delete all indexes containing the field before deleting the field
            final fieldName = operation.fieldName!;
            await migrationInstance.indexManager
                ?.removeIndex(currentTableName, fields: [fieldName]);
          }
        }

        // Process data migration based on migration type, collect records for cache
        if (renameOp != null && renameOp.newTableName != null) {
          // use high performance batch processing method instead of stream processing
          await migrationInstance.tableDataManager
              .rewriteRecordsFromSourceTable(
            sourceTableName: originalTableName,
            targetTableName: currentTableName,
            processFunction: (records, partitionIndex) async {
              // apply migration operations to records
              final modifiedRecords = await _applyMigrationOperations(
                  records, sortedOperations,
                  oldSchema: oldSchema);

              return modifiedRecords;
            },
          );
          // record partition migration performance data
          final migrationDuration = spaceStopwatch.elapsedMilliseconds;
          if (!SystemTable.isSystemTable(originalTableName)) {
            Logger.info(
              'Table rename batch processing performance: $migrationDuration ms for table $originalTableName -> $currentTableName',
              label: 'MigrationManager._executeMigrationTask',
            );
          }
        } else {
          if (needDataMigration) {
            // no rename table operation, directly process data migration
            // Use oldSchema to decode old data correctly before applying migration operations
            await migrationInstance.tableDataManager.processTablePartitions(
                tableName: currentTableName,
                decodeSchema:
                    oldSchema, // Critical: use old schema to decode old data
                processFunction: (records, partitionIndex, controller) async {
                  final migratedRecords = await _applyMigrationOperations(
                      records, sortedOperations,
                      oldSchema: oldSchema);

                  return migratedRecords;
                });
          }
        }

        // update task status
        final updatedTask = task.removePendingSpace(space);
        await _saveMigrationTask(updatedTask);

        // close migration instance
        await migrationInstance.close();

        spaceStopwatch.stop();
        Logger.info(
          'Space [$space] migration completed, time taken: ${spaceStopwatch.elapsedMilliseconds}ms',
          label: 'MigrationManager._executeMigrationTask',
        );
      }

      // async delete original table without waiting, avoid blocking migration process completion
      if (renameOp != null && renameOp.newTableName != null) {
        _dataStore.dropTable(originalTableName);
      } else {
        // if auto generated task, ensure schema consistency with async operation
        if (task.isAutoGenerated) {
          _asyncEnsureSchemaConsistency(task);
        }
      }

      // calculate and print total time
      taskStopwatch.stop();
      if (!SystemTable.isSystemTable(task.tableName)) {
        Logger.info(
          'Migration task [${task.taskId}] completed, total time: ${taskStopwatch.elapsedMilliseconds}ms',
          label: 'MigrationManager._executeMigrationTask',
        );
      }
    } catch (e, stack) {
      Logger.error(
        'Execute migration task failed: $e\n$stack',
        label: 'MigrationManager._executeMigrationTask',
      );
      rethrow;
    }
  }

  /// Sort operations to ensure they are executed in correct order
  List<MigrationOperation> _sortOperations(
      List<MigrationOperation> operations) {
    // Define operation type priority
    final typePriority = {
      MigrationType.setPrimaryKeyConfig: 1,
      MigrationType.renameTable: 2,
      MigrationType.addField: 3,
      MigrationType.modifyField: 4,
      MigrationType.renameField: 5,
      MigrationType.removeField: 6,
      MigrationType.addIndex: 7,
      MigrationType.modifyIndex: 8,
      MigrationType.removeIndex: 9,
      MigrationType.dropTable: 10,
    };

    // Sort operations by priority
    operations.sort((a, b) =>
        (typePriority[a.type] ?? 99).compareTo(typePriority[b.type] ?? 99));

    return operations;
  }

  /// Apply migration operations to records
  Future<List<Map<String, dynamic>>> _applyMigrationOperations(
      List<Map<String, dynamic>> records, List<MigrationOperation> operations,
      {TableSchema? oldSchema}) async {
    if (records.isEmpty || operations.isEmpty) {
      return records;
    }
    // Get max concurrent
    final maxConcurrent = _dataStore.config.maxConcurrency;

    // Batch size
    final int batchSize = (records.length / maxConcurrent).ceil();

    // Create batches
    final batches = <List<Map<String, dynamic>>>[];
    for (int i = 0; i < records.length; i += batchSize) {
      final end = min(i + batchSize, records.length);
      batches.add(records.sublist(i, end));
    }

    // Parallel processing all batches
    final batchResults =
        await Future.wait(batches.map((batch) => ComputeManager.run(
            processMigrationRecords,
            MigrationRecordProcessRequest(
              records: batch,
              operations: operations,
              oldSchema: oldSchema,
              yieldDurationMs: _dataStore.config.yieldDurationMs,
            ),
            useIsolate: records.length > 500)));

    // Merge results
    final allProcessedRecords = <Map<String, dynamic>>[];
    for (final batchResult in batchResults) {
      if (batchResult.success) {
        allProcessedRecords.addAll(batchResult.migratedRecords);
      } else {
        Logger.warn(
          'Batch migration failed: ${batchResult.errorMessage}',
          label: 'MigrationManager._applyMigrationOperations',
        );
        allProcessedRecords.addAll(batchResult.migratedRecords);
      }
    }

    return allProcessedRecords;
  }

  /// Cleanup task files after completion
  Future<void> _cleanupTask(MigrationTask task) async {
    try {
      final taskPath = _dataStore.pathManager
          .getMigrationTaskPath(task.dirIndex, task.taskId);

      // Check if file exists before attempting to delete
      final fileExists = await _dataStore.storage.existsFile(taskPath);
      if (fileExists) {
        await _dataStore.storage.deleteFile(taskPath);
      }

      // Always update meta data: remove task from directory mapping
      // This ensures mapping stays consistent even if file was already deleted
      final meta = await _getOrLoadMigrationMeta();

      // Verify task still exists in mapping before removing
      if (meta.directoryMapping.getDirIndex(task.taskId) != null) {
        final updatedMapping = meta.directoryMapping.removeId(task.taskId);
        final updatedMeta = meta.copyWith(directoryMapping: updatedMapping);
        await _saveMigrationMeta(updatedMeta);
      }
    } catch (e) {
      Logger.error(
        'Cleanup migration task failed: $e',
        label: 'MigrationManager._cleanupTask',
      );
    }
  }

  /// initialize migration manager, recover unfinished tasks
  Future<void> initialize() async {
    try {
      // check if global config has unfinished migration
      final globalConfig = await _dataStore.getGlobalConfig();
      if (globalConfig != null && globalConfig.hasMigrationTask) {
        // load migration meta data
        final meta = await _getOrLoadMigrationMeta();
        if (meta.directoryMapping.idToDir.isNotEmpty) {
          // Track tasks to remove from mapping (file doesn't exist)
          final tasksToRemove = <String>[];

          // load unfinished tasks one by one
          for (final entry in meta.directoryMapping.idToDir.entries) {
            final taskId = entry.key;
            final dirIndex = entry.value;

            try {
              // Check if task file exists
              final taskPath =
                  _dataStore.pathManager.getMigrationTaskPath(dirIndex, taskId);
              final fileExists = await _dataStore.storage.existsFile(taskPath);

              if (!fileExists) {
                // File doesn't exist but mapping has it - mark for cleanup
                Logger.warn(
                  'Task file not found but exists in mapping: taskId[$taskId], dirIndex[$dirIndex], cleaning up mapping',
                  label: 'MigrationManager.initialize',
                );
                tasksToRemove.add(taskId);
                continue;
              }

              // read task file
              final taskContent =
                  await _dataStore.storage.readAsString(taskPath);

              if (taskContent != null && taskContent.isNotEmpty) {
                // parse task
                final task = MigrationTask.fromJson(jsonDecode(taskContent));

                // check if there are remaining spaces to be processed
                if (task.pendingMigrationSpaces.isNotEmpty) {
                  Logger.info(
                    'Found unfinished migration task: taskId[${task.taskId}], table[${task.tableName}], remaining spaces[${task.pendingMigrationSpaces.length}]',
                    label: 'MigrationManager.initialize',
                  );

                  // add to pending tasks queue
                  _pendingTasks.add(task);
                } else {
                  // Task is completed but still in mapping - mark for cleanup
                  Logger.info(
                    'Task completed but still in mapping: taskId[$taskId], cleaning up mapping',
                    label: 'MigrationManager.initialize',
                  );
                  tasksToRemove.add(taskId);
                }
              } else {
                // File exists but is empty - mark for cleanup
                Logger.warn(
                  'Task file is empty: taskId[$taskId], dirIndex[$dirIndex], cleaning up mapping',
                  label: 'MigrationManager.initialize',
                );
                tasksToRemove.add(taskId);
              }
            } catch (e) {
              Logger.warn(
                'Failed to load task: taskId[$taskId], error[$e], cleaning up mapping',
                label: 'MigrationManager.initialize',
              );
              tasksToRemove.add(taskId);
            }
          }

          // Clean up orphaned mappings (tasks that don't exist or are completed)
          if (tasksToRemove.isNotEmpty) {
            await _cleanupOrphanedMappings(tasksToRemove);
          }

          // start processing recovered tasks
          if (_pendingTasks.isNotEmpty) {
            final success = await processMigrationTasks();
            if (!success) {
              Logger.warn(
                'Some recovered migration tasks failed, please check the log for details',
                label: 'MigrationManager.initialize',
              );
            }
          }
        }
      }
    } catch (e, stack) {
      Logger.error(
        'Failed to initialize migration manager: $e\n$stack',
        label: 'MigrationManager.initialize',
      );
    }
  }

  /// Clean up orphaned task mappings (tasks that no longer exist)
  Future<void> _cleanupOrphanedMappings(List<String> taskIds) async {
    try {
      final meta = await _getOrLoadMigrationMeta();
      var updatedMapping = meta.directoryMapping;

      // Remove each orphaned task from mapping
      for (final taskId in taskIds) {
        updatedMapping = updatedMapping.removeId(taskId);
      }

      // Only save if mapping changed
      if (updatedMapping.idToDir.length !=
          meta.directoryMapping.idToDir.length) {
        final updatedMeta = meta.copyWith(directoryMapping: updatedMapping);
        await _saveMigrationMeta(updatedMeta);

        Logger.info(
          'Cleaned up ${taskIds.length} orphaned task mapping(s)',
          label: 'MigrationManager._cleanupOrphanedMappings',
        );
      }
    } catch (e) {
      Logger.error(
        'Failed to cleanup orphaned mappings: $e',
        label: 'MigrationManager._cleanupOrphanedMappings',
      );
    }
  }

  /// check if specified space is being migrated
  Future<bool> isSpaceBeingMigrated(String spaceName) async {
    for (final task in _pendingTasks) {
      if (task.pendingMigrationSpaces.contains(spaceName)) {
        return true;
      }
    }
    return false;
  }

  /// wait for migration of specified space to complete
  Future<bool> waitForSpaceMigration(String spaceName,
      {Duration? timeout}) async {
    // create a completer to handle asynchronous waiting
    final completer = Completer<bool>();

    // create a timer to check migration status
    Timer? checkTimer;
    Timer? timeoutTimer;

    // set timeout
    if (timeout != null) {
      timeoutTimer = Timer(timeout, () {
        if (!completer.isCompleted) {
          checkTimer?.cancel();
          completer.complete(false);
        }
      });
    }

    // check migration status periodically
    checkTimer = Timer.periodic(const Duration(seconds: 1), (timer) async {
      final isStillMigrating = await isSpaceBeingMigrated(spaceName);
      if (!isStillMigrating && !completer.isCompleted) {
        timer.cancel();
        timeoutTimer?.cancel();
        completer.complete(true);
      }
    });

    return completer.future;
  }

  /// query migration task status
  ///
  /// [taskId] task ID
  /// return task details, null if task does not exist
  Future<MigrationStatus?> queryTaskStatus(String taskId) async {
    try {
      // load migration meta data
      final meta = await _getOrLoadMigrationMeta();
      final dirIndex = meta.directoryMapping.getDirIndex(taskId);

      if (dirIndex == null) {
        // task completed or not exist
        return MigrationStatus(
          taskId: taskId,
          isCompleted: true, // task ID not in mapping, consider as completed
          createTime: DateTime.now(),
          pendingSpaces: const [],
          processedSpacesCount: 0,
          totalSpacesCount: 0,
        );
      }

      // read task file
      final taskPath =
          _dataStore.pathManager.getMigrationTaskPath(dirIndex, taskId);

      // Check if file exists
      final fileExists = await _dataStore.storage.existsFile(taskPath);
      if (!fileExists) {
        // File doesn't exist but mapping has it - cleanup mapping
        Logger.warn(
          'Task file not found but exists in mapping: taskId[$taskId], dirIndex[$dirIndex], cleaning up mapping',
          label: 'MigrationManager.queryTaskStatus',
        );
        await _cleanupOrphanedMappings([taskId]);
        return MigrationStatus(
          taskId: taskId,
          isCompleted: true,
          createTime: DateTime.now(),
          pendingSpaces: const [],
          processedSpacesCount: 0,
          totalSpacesCount: 0,
        );
      }

      final taskContent = await _dataStore.storage.readAsString(taskPath);

      if (taskContent == null || taskContent.isEmpty) {
        // File exists but is empty - cleanup mapping
        Logger.warn(
          'Task file is empty: taskId[$taskId], dirIndex[$dirIndex], cleaning up mapping',
          label: 'MigrationManager.queryTaskStatus',
        );
        await _cleanupOrphanedMappings([taskId]);
        return null;
      }

      // parse task information
      final taskJson = jsonDecode(taskContent);
      final task = MigrationTask.fromJson(taskJson);

      // get processed and pending spaces information
      final allSpaces = await _getAllSpaces();
      final pendingSpaces = task.pendingMigrationSpaces;
      final processedSpaces =
          allSpaces.where((space) => !pendingSpaces.contains(space)).length;

      return MigrationStatus(
        taskId: taskId,
        isCompleted: pendingSpaces.isEmpty,
        createTime: task.createTime,
        pendingSpaces: pendingSpaces,
        processedSpacesCount: processedSpaces,
        totalSpacesCount: allSpaces.length,
      );
    } catch (e) {
      Logger.error(
        'Failed to query task status: $e',
        label: 'MigrationManager.queryTaskStatus',
      );
      return null;
    }
  }

  /// Asynchronously ensure schema consistency for auto-generated tasks
  void _asyncEnsureSchemaConsistency(MigrationTask task) {
    // Execute asynchronously without waiting
    Future(() async {
      try {
        // Get the final table name (considering table renaming)
        String targetTableName = task.tableName;

        // Check if there's a rename table operation
        MigrationOperation? renameOp;
        try {
          renameOp = task.operations.firstWhere(
            (op) => op.type == MigrationType.renameTable,
          );
          // If there's a rename operation, use the new table name
          if (renameOp.newTableName != null) {
            targetTableName = renameOp.newTableName!;
          }
        } catch (e) {
          // No rename operation found, continue using original table name
        }

        // Get both schemas for comparison
        final initialSchema = _dataStore.getInitialSchemas();
        final definitionSchema = initialSchema.firstWhere(
          (s) => s.name == targetTableName,
          orElse: () => throw Exception('Schema not found in initial schema'),
        );

        final currentSchema =
            await _dataStore.schemaManager?.getTableSchema(targetTableName);

        // Convert schemas to JSON for deep comparison
        final definitionJson = jsonEncode(definitionSchema.toJson());
        final currentJson =
            currentSchema != null ? jsonEncode(currentSchema.toJson()) : null;

        // Only update if schemas are different
        if (currentJson == null || definitionJson != currentJson) {
          await _dataStore.schemaManager!
              .saveTableSchema(targetTableName, definitionSchema);
        } else {
          Logger.debug(
            'Schema for table [$targetTableName] is already consistent with definition',
            label: 'MigrationManager._asyncEnsureSchemaConsistency',
          );
        }
      } catch (e, stack) {
        Logger.error(
          'Async schema consistency check failed: $e\n$stack',
          label: 'MigrationManager._asyncEnsureSchemaConsistency',
        );
      }
    });
  }

  /// check if the migration operations require data migration.
  Future<bool> _requiresDataMigration(
      List<MigrationOperation> operations, TableSchema oldSchema) async {
    for (final op in operations) {
      switch (op.type) {
        case MigrationType.addField:
          final field = op.field!;
          if (!field.nullable &&
              field.defaultValue == null &&
              field.defaultValueType == DefaultValueType.none) {
            Logger.warn(
              'Data migration required: adding non-nullable field "${field.name}" without a default value.',
              label: 'MigrationManager._requiresDataMigration',
            );
            return true;
          }
          break;
        case MigrationType.modifyField:
          final fieldUpdate = op.fieldUpdate!;
          FieldSchema? oldField;
          for (final f in oldSchema.fields) {
            if (f.name == fieldUpdate.name) {
              oldField = f;
              break;
            }
          }

          if (oldField != null) {
            // from nullable to non-nullable
            if (oldField.nullable && (fieldUpdate.nullable == false)) {
              Logger.warn(
                'Data migration required: changing field "${fieldUpdate.name}" from nullable to non-nullable.',
                label: 'MigrationManager._requiresDataMigration',
              );
              return true;
            }
            // from non-unique to unique
            if (!oldField.unique && (fieldUpdate.unique == true)) {
              Logger.warn(
                'Data migration required: changing field "${fieldUpdate.name}" from non-unique to unique.',
                label: 'MigrationManager._requiresDataMigration',
              );
              return true;
            }
          }
          break;
        case MigrationType.setPrimaryKeyConfig:
          final newConfig = op.primaryKeyConfig;
          final oldConfig = oldSchema.primaryKeyConfig;
          if (newConfig != null &&
              (newConfig.name != oldConfig.name ||
                  newConfig.type != oldConfig.type)) {
            Logger.warn(
              'Data migration required: changing primary key name or type.',
              label: 'MigrationManager._requiresDataMigration',
            );
            return true;
          }
          break;
        default:
          break;
      }
    }
    return false;
  }

  /// Check for orphaned records before removing foreign key
  ///
  /// Orphaned records are records in the referencing table that reference
  /// records in the referenced table that no longer exist (or will no longer
  /// be validated after foreign key removal).
  ///
  /// This method checks for potential data integrity issues and logs warnings.
  /// It does not prevent the foreign key removal, but alerts the developer
  /// to potential data quality issues.
  Future<void> _checkOrphanedRecordsBeforeRemovingForeignKey(
    String tableName,
    ForeignKeySchema fk,
  ) async {
    try {
      // Check if referenced table exists
      final referencedSchema =
          await _dataStore.schemaManager?.getTableSchema(fk.referencedTable);
      if (referencedSchema == null) {
        Logger.warn(
          'Referenced table ${fk.referencedTable} does not exist. '
          'Removing foreign key ${fk.actualName} from table $tableName. '
          'All records in $tableName that reference ${fk.referencedTable} are now orphaned.',
          label:
              'MigrationManager._checkOrphanedRecordsBeforeRemovingForeignKey',
        );
        return;
      }

      // Build query to find records in referencing table
      // We need to check if any records have foreign key values that don't exist in referenced table
      final queryBuilder = QueryBuilder(_dataStore, tableName);

      // For composite foreign keys, we need to check all fields
      // For simplicity, we'll sample a few records to check
      // In production, you might want to do a full scan or use a more efficient method

      // Get a sample of records to check
      final sampleResults = await queryBuilder.limit(100).future;

      if (sampleResults.data.isEmpty) {
        // No records to check
        return;
      }

      // Check each sample record for orphaned references
      int orphanedCount = 0;
      final orphanedRecords = <Map<String, dynamic>>[];

      for (final record in sampleResults.data) {
        // Build condition to check if referenced record exists
        final refCondition = <String, dynamic>{};
        bool hasNonNullValue = false;

        for (int i = 0; i < fk.fields.length; i++) {
          final fkField = fk.fields[i];
          final refField = fk.referencedFields[i];
          final fkValue = record[fkField];

          if (fkValue != null) {
            hasNonNullValue = true;
            refCondition[refField] = fkValue;
          }
        }

        if (hasNonNullValue && refCondition.isNotEmpty) {
          // Check if referenced record exists
          final refQueryBuilder = QueryBuilder(_dataStore, fk.referencedTable);
          for (final entry in refCondition.entries) {
            refQueryBuilder.where(entry.key, '=', entry.value);
          }
          final refResults = await refQueryBuilder.limit(1).future;

          if (refResults.data.isEmpty) {
            orphanedCount++;
            if (orphanedRecords.length < 10) {
              // Keep sample of orphaned records for logging
              orphanedRecords.add(record);
            }
          }
        }
      }

      if (orphanedCount > 0) {
        Logger.warn(
          'Found $orphanedCount orphaned record(s) in sample when removing foreign key ${fk.actualName} from table $tableName. '
          'These records reference non-existent records in table ${fk.referencedTable}. '
          'After removing the foreign key, these records will no longer be validated, which may cause data integrity issues.\n'
          'Sample orphaned records: ${orphanedRecords.take(3).map((r) => r.toString()).join(", ")}\n'
          'Consider cleaning up orphaned records before or after removing the foreign key.',
          label:
              'MigrationManager._checkOrphanedRecordsBeforeRemovingForeignKey',
        );
      } else {
        Logger.info(
          'No orphaned records detected in sample when removing foreign key ${fk.actualName} from table $tableName.',
          label:
              'MigrationManager._checkOrphanedRecordsBeforeRemovingForeignKey',
        );
      }
    } catch (e) {
      // Don't throw - just log warning
      // Foreign key removal should proceed even if orphaned record check fails
      Logger.warn(
        'Failed to check for orphaned records before removing foreign key ${fk.actualName}: $e',
        label: 'MigrationManager._checkOrphanedRecordsBeforeRemovingForeignKey',
      );
    }
  }
}

/// table rename detection result
class RenamedTableResult {
  /// rename table mapping {old table name: new table structure}
  final Map<String, TableSchema> renamedTables;

  /// tables to create
  final List<String> tablesToCreate;

  /// tables to drop
  final List<String> tablesToDrop;

  const RenamedTableResult({
    required this.renamedTables,
    required this.tablesToCreate,
    required this.tablesToDrop,
  });
}
